{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='background:#222;color:white;text-align:center'>Off line part</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Example usage:\\ninput_file_path = \\'sample.txt\\'\\noutput_folder_path = \\'docs/\\'\\ntext_identifier = \\'TEXT:\\'\\n\\nresulting_docs = FileHandler().extract_and_save_text(input_file_path, output_folder_path, text_identifier)\\nprint(\"Documents created:\", resulting_docs)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FileHandler:\n",
    "    @staticmethod\n",
    "    def read_and_print(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                print(content)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_and_save_text(input_file, output_folder, text_prefix=\"\"):\n",
    "        with open(input_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        doc_number = 1\n",
    "        doc_list = []\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith(text_prefix):\n",
    "                file_path = os.path.join(output_folder, f'doc{doc_number}_.txt')\n",
    "                with open(file_path, 'w') as new_file:\n",
    "                    new_file.write(line[len(text_prefix):])  # Write the content after the text prefix\n",
    "                doc_list.append(file_path)\n",
    "                doc_number += 1\n",
    "\n",
    "        return doc_list\n",
    "\n",
    "\"\"\"# Example usage:\n",
    "input_file_path = 'sample.txt'\n",
    "output_folder_path = 'docs/'\n",
    "text_identifier = 'TEXT:'\n",
    "\n",
    "resulting_docs = FileHandler().extract_and_save_text(input_file_path, output_folder_path, text_identifier)\n",
    "print(\"Documents created:\", resulting_docs)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Example usage:\\nfile_path = \"./sample.txt\"\\noutput_file_name = \"collection1.txt\"\\noutput_folder_path = \"./\"\\npreprocessor = TextPreprocessor(file_path, output_file_name, output_folder_path)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, file_path, output_file_name, output_folder_path, tokenize=True, remove_stopwords=True, stemming=True):\n",
    "        self.file_path = file_path\n",
    "        self.output_file_name = output_file_name\n",
    "        self.output_folder_path = output_folder_path\n",
    "        self.tokenize = tokenize\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stemming = stemming\n",
    "        #---\n",
    "        self.text = None\n",
    "        self.tokens = []  # Initialize as an empty list\n",
    "        self.filtered_tokens = []  # Initialize as an empty list\n",
    "        self.stemmed_tokens = []  # Initialize as an empty list\n",
    "        #---\n",
    "        self.porter_stemmer = PorterStemmer() if stemming else None  # Initialize stemmer conditionally\n",
    "        try:\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        self.preprocess_text()\n",
    "    #----------Functions------------\n",
    "    def read_text(self):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            self.text = file.read()\n",
    "\n",
    "    def tokenize_text(self):\n",
    "        if self.text is not None:\n",
    "            self.tokens = word_tokenize(self.text)\n",
    "            self.tokens = [word.lower() for word in self.tokens if word.isalnum()]\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        if self.tokens:\n",
    "            self.filtered_tokens = [word for word in self.tokens if word not in self.stop_words]\n",
    "\n",
    "    def porter_stemming(self):\n",
    "        if self.stemming and self.filtered_tokens and self.porter_stemmer:  # Check if stemming is enabled\n",
    "            self.stemmed_tokens = [self.porter_stemmer.stem(word) for word in self.filtered_tokens]\n",
    "        else:\n",
    "            self.stemmed_tokens = self.filtered_tokens[:]  # If stemming is disabled, copy the tokens as they are\n",
    "\n",
    "    def save_processed_text(self):\n",
    "        if self.stemmed_tokens:  # Check if list is not empty\n",
    "            processed_text = \" \".join(self.stemmed_tokens)\n",
    "        else:\n",
    "            processed_text = \" \".join(self.filtered_tokens) if self.filtered_tokens else \"\"\n",
    "        output_path = os.path.join(self.output_folder_path, self.output_file_name)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(processed_text)\n",
    "\n",
    "    def preprocess_text(self):\n",
    "        self.read_text()\n",
    "        self.tokenize_text()\n",
    "        self.remove_stop_words()\n",
    "        self.porter_stemming()\n",
    "        self.save_processed_text()\n",
    "\n",
    "'''# Example usage:\n",
    "file_path = \"./sample.txt\"\n",
    "output_file_name = \"collection1.txt\"\n",
    "output_folder_path = \"./\"\n",
    "preprocessor = TextPreprocessor(file_path, output_file_name, output_folder_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalInvertedIndex:\n",
    "    def __init__(self,folder_path,output_folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.output_folder_path = output_folder_path\n",
    "        self.index = {}\n",
    "\n",
    "    def add_term(self, term, document_id, position):\n",
    "        if term not in self.index:\n",
    "            self.index[term] = {'doc_freq': 0, 'postings': {}}\n",
    "        if document_id not in self.index[term]['postings']:\n",
    "            self.index[term]['postings'][document_id] = []\n",
    "            self.index[term]['doc_freq'] += 1\n",
    "        self.index[term]['postings'][document_id].append(position)\n",
    "\n",
    "    def construct_index(self):\n",
    "        files = os.listdir(self.folder_path)\n",
    "        doc_id = 1\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(self.folder_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                preprocessor = TextPreprocessor(file_path, f\"afterPrDoc{doc_id}_.txt\", self.output_folder_path)\n",
    "                preprocessor.preprocess_text()\n",
    "                for position, term in enumerate(preprocessor.stemmed_tokens):\n",
    "                    self.add_term(term, file, position)\n",
    "                doc_id += 1\n",
    "                \n",
    "    def print_index(self):\n",
    "        for term, postings in self.index.items():\n",
    "            print(f\" \\n Term: '{term}'  Document Frequency: {postings['doc_freq']} \")\n",
    "            print(\" Postings:\")\n",
    "            for doc_id, positions in postings['postings'].items():\n",
    "                print(f\"   {doc_id} :\")\n",
    "                print(f\"   Positions: {', '.join(str(pos) for pos in positions)}\")\n",
    "            print()\n",
    "    def save_index_as_pickle(self, file_path):\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(self.index, file)       \n",
    "\n",
    "# Usage example:\n",
    "index = PositionalInvertedIndex('./docs','./docs/afterProcssDocs')\n",
    "index.construct_index()\n",
    "\n",
    "# Accessing the constructed index:\n",
    "#print(index.index)\n",
    "#index.print_index()\n",
    "index.save_index_as_pickle('./indexes/index.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='background:#222;color:white;text-align:center'>Online part</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GAZA', 'TO', 'Egybt']\n",
      "['gaza', 'egybt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Query:\n",
    "    def __init__(self, words):\n",
    "        if isinstance(words, str):\n",
    "            words = [words]  # Convert a single word string to a list\n",
    "        self.words = words\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "    \n",
    "    def remove_stop_words(self):\n",
    "        if self.words is None:\n",
    "            return []\n",
    "\n",
    "        final_words = []\n",
    "        lower_stop_words = set(map(str.lower, self.stop_words))\n",
    "        for word in self.words:\n",
    "            if (word.lower() not in lower_stop_words):\n",
    "                final_words.append(word)\n",
    "\n",
    "        return final_words\n",
    "\n",
    "\n",
    "    def stem_words(self , words):\n",
    "        if  words is None:\n",
    "            return []\n",
    "\n",
    "        return [self.stemmer.stem(word) for word in  words]\n",
    "\n",
    "    def process_text(self, remove_stopwords=True, stemming=True):\n",
    "\n",
    "        if remove_stopwords:\n",
    "            processed = self.remove_stop_words()\n",
    "             \n",
    "\n",
    "        if stemming:\n",
    "            processed = self.stem_words(processed)\n",
    "\n",
    "        return processed\n",
    "\n",
    "# Example usage\n",
    "phrase_terms = ['GAZA', 'TO', 'Egybt']\n",
    "print(phrase_terms)\n",
    "\n",
    "query = Query(phrase_terms)\n",
    "processed_terms = query.process_text(remove_stopwords=True, stemming=True)\n",
    "print(processed_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents matching the query: ['paragraph_48.txt', 'paragraph_33.txt', '01 (8th copy).txt', 'paragraph_29.txt', '01 (6th copy).txt', '_paragraph_12.txt', 'paragraph_2.txt', '_paragraph_52.txt', '_paragraph_43.txt', '01 (34th copy).txt', 'paragraph_1.txt', '_paragraph_48.txt', 'paragraph_52.txt', '_paragraph_2.txt', 'paragraph_12.txt', 'tweet_143.txt', '01 (13th copy).txt', 'paragraph_43.txt', '_paragraph_51.txt', 'paragraph_51.txt', 'paragraph_42.txt', 'tweet_196.txt', '_paragraph_33.txt', '_paragraph_29.txt', '01 (12th copy).txt'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class BooleanSearch:\n",
    "    @staticmethod\n",
    "    def load_index_from_pickle(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def boolean_search(query, positional_index):\n",
    "        query_terms = query.split()\n",
    "        and_terms = [term for term in query_terms if term != 'AND' and term != 'OR' and term != 'NOT']\n",
    "        operators = [term for term in query_terms if term == 'AND' or term == 'OR' or term == 'NOT']\n",
    "        \n",
    "        query = Query(and_terms)\n",
    "        and_terms = query.process_text(remove_stopwords=True, stemming=True)\n",
    "        \n",
    "        result = set(positional_index.get(and_terms[0], {}).get('postings', {}).keys())\n",
    "\n",
    "        for i in range(len(operators)):\n",
    "            if operators[i] == 'AND':\n",
    "                term_docs = set(positional_index.get(and_terms[i + 1], {}).get('postings', {}).keys())\n",
    "                result = result.intersection(term_docs)\n",
    "            elif operators[i] == 'OR':\n",
    "                term_docs = set(positional_index.get(and_terms[i + 1], {}).get('postings', {}).keys())\n",
    "                result = result.union(term_docs)\n",
    "            elif operators[i] == 'NOT':\n",
    "                term_docs = set(positional_index.get(and_terms[i + 1], {}).get('postings', {}).keys())\n",
    "                result = result.difference(term_docs)\n",
    "\n",
    "        return list(result)\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "index_file_path = \"./indexes/index.pkl\"  # Replace with your .pkl file path\n",
    "positional_index = BooleanSearch.load_index_from_pickle(index_file_path)\n",
    "\n",
    "query = \"Hamas AND GAZA\"\n",
    "result_docs = BooleanSearch.boolean_search(query, positional_index)\n",
    "print(\"Documents matching the query:\", result_docs,\"\\n\")\n",
    "\n",
    "#if you want to see the content of those docs run this code\n",
    "#for file in result_docs:\n",
    "    #FileHandler().read_and_print('./docs/'+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase 'Palestinian boy' found in the following positions:\n",
      "- Document: tweet_25.txt, Starting Position: 2\n",
      "Awni Eldous: The Palestinian boy who found YouTube fame after death\n",
      "https://www.bbc.co.uk/news/world-middle-east-67788360\n"
     ]
    }
   ],
   "source": [
    "class PhraseSearch:\n",
    "    def __init__(self, index_file):\n",
    "        self.index = self.load_index(index_file)\n",
    "\n",
    "    def load_index(self, index_file):\n",
    "        with open(index_file, 'rb') as file:\n",
    "           return pickle.load(file)\n",
    "\n",
    "    def phrase_search(self, query):\n",
    "        and_terms = query.split()\n",
    "        query = Query(and_terms)\n",
    "        terms = query.process_text(remove_stopwords=True, stemming=True)\n",
    "\n",
    "        # Initialize a dictionary to store matching documents and positions\n",
    "        phrase_positions = {}\n",
    "\n",
    "        # Get postings for the first term in the query\n",
    "        first_term = terms[0]\n",
    "        if first_term in self.index:\n",
    "            first_term_postings = self.index[first_term]['postings']\n",
    "\n",
    "            # Loop through each document containing the first term\n",
    "            for doc, positions in first_term_postings.items():\n",
    "                # Check for the presence of subsequent terms in order\n",
    "                pos = positions[0]  # Consider the first position of the first term\n",
    "\n",
    "                # Check if the remaining terms exist in consecutive positions in the document\n",
    "                for i in range(1, len(terms)):\n",
    "                    next_term = terms[i]\n",
    "                    if doc in self.index.get(next_term, {}).get('postings', {}):\n",
    "                        next_term_positions = self.index[next_term]['postings'][doc]\n",
    "                        if pos + i in next_term_positions:\n",
    "                            pos = pos + i  # Move to the next position in the document\n",
    "                        else:\n",
    "                            break  # Move to the next document if positions are not consecutive\n",
    "                    else:\n",
    "                        break  # Move to the next document if the term is not found\n",
    "\n",
    "                else:  # If all terms are found in order in the document\n",
    "                    if doc not in phrase_positions:\n",
    "                        phrase_positions[doc] = [pos - len(terms) + 1]\n",
    "                    else:\n",
    "                        phrase_positions[doc].append(pos - len(terms) + 1)\n",
    "\n",
    "        return phrase_positions\n",
    "# Example usage:\n",
    "index_file_path = './indexes/index.pkl'  # Replace this with your file path\n",
    "phrase_searcher = PhraseSearch(index_file_path)\n",
    "\n",
    "# Perform a phrase search for a query\n",
    "query = \"Palestinian boy\"\n",
    "results = phrase_searcher.phrase_search(query)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    print(f\"Phrase '{query}' found in the following positions:\")\n",
    "    for doc, positions in results.items():\n",
    "         \n",
    "        \n",
    "        for pos in positions:\n",
    "            print(f\"- Document: {doc}, Starting Position: {pos}\")\n",
    "            FileHandler().read_and_print('./docs/'+doc)\n",
    "else:\n",
    "    print(f\"No matches found for the phrase '{query}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('paragraph_1.txt', 51.0), ('paragraph_2.txt', 8.0), ('_paragraph_2.txt', 8.0), ('_paragraph_48.txt', 6.0), ('paragraph_48.txt', 6.0), ('01 (34th copy).txt', 3.0), ('paragraph_14.txt', 3.0), ('paragraph_33.txt', 3.0), ('_paragraph_33.txt', 3.0), ('_paragraph_14.txt', 3.0), ('paragraph_51.txt', 2.0), ('paragraph_44.txt', 2.0), ('_paragraph_44.txt', 2.0), ('paragraph_13.txt', 2.0), ('paragraph_34.txt', 2.0), ('paragraph_46.txt', 2.0), ('_paragraph_46.txt', 2.0), ('_paragraph_13.txt', 2.0), ('paragraph_41.txt', 2.0), ('_paragraph_51.txt', 2.0), ('paragraph_52.txt', 2.0), ('paragraph_42.txt', 2.0), ('_paragraph_41.txt', 2.0), ('_paragraph_52.txt', 2.0), ('paragraph_36.txt', 2.0), ('tweet_99.txt', 1.0), ('tweet_202.txt', 1.0), ('tweet_96.txt', 1.0), ('tweet_117.txt', 1.0), ('_paragraph_31.txt', 1.0), ('tweet_78.txt', 1.0), ('tweet_1.txt', 1.0), ('tweet_120.txt', 1.0), ('tweet_21.txt', 1.0), ('tweet_186.txt', 1.0), ('tweet_164.txt', 1.0), ('01 (21st copy).txt', 1.0), ('paragraph_31.txt', 1.0), ('tweet_169.txt', 1.0), ('paragraph_4.txt', 1.0), ('tweet_104.txt', 1.0), ('tweet_144.txt', 1.0), ('_paragraph_23.txt', 1.0), ('tweet_51.txt', 1.0), ('01 (36th copy).txt', 1.0), ('tweet_79.txt', 1.0), ('tweet_162.txt', 1.0), ('tweet_138.txt', 1.0), ('tweet_132.txt', 1.0), ('_paragraph_37.txt', 1.0), ('tweet_6.txt', 1.0), ('tweet_92.txt', 1.0), ('tweet_103.txt', 1.0), ('tweet_142.txt', 1.0), ('tweet_17.txt', 1.0), ('01 (9th copy).txt', 1.0), ('paragraph_22.txt', 1.0), ('_paragraph_22.txt', 1.0), ('paragraph_12.txt', 1.0), ('tweet_73.txt', 1.0), ('tweet_57.txt', 1.0), ('tweet_185.txt', 1.0), ('tweet_178.txt', 1.0), ('tweet_145.txt', 1.0), ('tweet_105.txt', 1.0), ('tweet_200.txt', 1.0), ('tweet_199.txt', 1.0), ('tweet_125.txt', 1.0), ('tweet_36.txt', 1.0), ('paragraph_37.txt', 1.0), ('tweet_87.txt', 1.0), ('tweet_12.txt', 1.0), ('tweet_83.txt', 1.0), ('01 (10th copy).txt', 1.0), ('tweet_75.txt', 1.0), ('01 (3rd copy).txt', 1.0), ('paragraph_23.txt', 1.0), ('tweet_131.txt', 1.0), ('01 (22nd copy).txt', 1.0), ('tweet_137.txt', 1.0), ('tweet_76.txt', 1.0), ('tweet_129.txt', 1.0), ('tweet_182.txt', 1.0), ('_paragraph_12.txt', 1.0), ('01 (7th copy).txt', 1.0), ('tweet_44.txt', 1.0), ('tweet_3.txt', 1.0), ('tweet_152.txt', 1.0), ('tweet_115.txt', 1.0), ('01 (38th copy).txt', 1.0), ('_paragraph_4.txt', 1.0), ('tweet_100.txt', 1.0), ('tweet_30.txt', 1.0), ('tweet_14.txt', 1.0), ('tweet_122.txt', 1.0), ('tweet_106.txt', 1.0)] Ranked Documents based on modified TF-IDF scores:\n",
      "Document: paragraph_1.txt, TF-IDF Score: 51.0\n",
      "Document: paragraph_2.txt, TF-IDF Score: 8.0\n",
      "Document: _paragraph_2.txt, TF-IDF Score: 8.0\n",
      "Document: _paragraph_48.txt, TF-IDF Score: 6.0\n",
      "Document: paragraph_48.txt, TF-IDF Score: 6.0\n",
      "Document: 01 (34th copy).txt, TF-IDF Score: 3.0\n",
      "Document: paragraph_14.txt, TF-IDF Score: 3.0\n",
      "Document: paragraph_33.txt, TF-IDF Score: 3.0\n",
      "Document: _paragraph_33.txt, TF-IDF Score: 3.0\n",
      "Document: _paragraph_14.txt, TF-IDF Score: 3.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class TFIDF:\n",
    "    def __init__(self, inverted_index_path):\n",
    "        with open(inverted_index_path, 'rb') as file:\n",
    "            self.inverted_index = pickle.load(file)\n",
    "        self.total_docs = len(self.inverted_index[next(iter(self.inverted_index))]['postings'])\n",
    "        self.document_frequency = {term: self.inverted_index[term]['doc_freq'] for term in self.inverted_index}\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        query_terms_l = query.split()\n",
    "        query_c = Query(query_terms_l)  # Assuming Query class isn't defined in the provided code\n",
    "        processed_query = query_terms_l  # Skipping preprocessing for demonstration\n",
    "        return processed_query\n",
    "\n",
    "    def calculate_tfidf(self, query):\n",
    "        query_terms = self.preprocess_query(query)\n",
    "        term_frequency = defaultdict(lambda: defaultdict(int))\n",
    "        idf = {}\n",
    "        tfidf = defaultdict(float)\n",
    "\n",
    "        # Calculate term frequency in the query\n",
    "        query_term_freq = defaultdict(int)\n",
    "        for term in query_terms:\n",
    "            query_term_freq[term] += 1\n",
    "\n",
    "        # Calculate TF-IDF for each term in the query\n",
    "        for term, query_freq in query_term_freq.items():\n",
    "            if term in self.inverted_index:\n",
    "                idf[term] = 1 + math.log10(query_freq) * math.log10(self.total_docs / self.document_frequency[term])\n",
    "                for doc, positions in self.inverted_index[term]['postings'].items():\n",
    "                    term_frequency[term][doc] = len(positions)\n",
    "                    tfidf[doc] += term_frequency[term][doc] * idf[term]\n",
    "\n",
    "        # Normalize TF-IDF scores\n",
    "        #for doc in tfidf:\n",
    "          #  tfidf[doc] /= len(doc.split('_')[0])  # Normalize by the length of the document name\n",
    "\n",
    "        # Rank documents based on TF-IDF scores\n",
    "        ranked_docs = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "        return ranked_docs\n",
    "\n",
    "# Example usage:\n",
    "inverted_index_path = './indexes/index.pkl'\n",
    "tfidf_model = TFIDF(inverted_index_path)\n",
    "\n",
    "query = \"Hamas war\"\n",
    "ranked_documents = tfidf_model.calculate_tfidf(query)\n",
    "print(ranked_documents, \"Ranked Documents based on modified TF-IDF scores:\")\n",
    "i=0\n",
    "for doc, score in ranked_documents:\n",
    "    print(f\"Document: {doc}, TF-IDF Score: {score}\")\n",
    "    i=i+1\n",
    "    if (i==10):\n",
    "        break\n",
    "    #FileHandler().read_and_print('./docs/'+doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
